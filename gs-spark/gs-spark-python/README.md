# Getting Started with Spark using Python and Jupyter Notebook

Sample code to play with Spark using Python and Jupyter Notebook.

## Required Software and Setup

* Install Anaconda with Python 3.8 or latest version
* Setup Spark (e.g. spark-3.3.1-bin-hadoop3.2) and configure necessary environment variables (e.g. 'SPARK_HOME' and include '%SPARK_HOME%/bin' in path)  - [see](https://had00ping.wordpress.com/2017/11/14/setting-intellij-for-pyspark/)
* In case of Window OS, additionally setup Hadoop winutils and configure related environment variable (e.g. 'HADOOP_HOME' and include '%HADOOP_HOME%/bin' in 'path')
* Install Intellij 2018.3 (or latest) preferably with Python Plugin and open the 'gs-spark-python' project in IDE

## Word Count Example using Spark RDD

To be added...
    
## Getting Started with Spark using Python and Jupyter Notebook

See [gs-spark/gs-spark-python/notebooks](https://github.com/tirthalpatel/Learning-BigData/tree/master/gs-spark/gs-spark-python/notebooks) for understanding

* Spark 2 programming concepts
* Spark RDD for data exploration, preparation and analysis i.e. cleaning, transforming and summarizing data
* Spark DataFrames and Spark SQL for exploring, analyzing and querying data
* Spark Streaming (DStreams) concepts
