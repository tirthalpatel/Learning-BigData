{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime Analysis in New York City using Spark DataFrame Example \n",
    "\n",
    "__Learning Objective:__\n",
    "\n",
    "* [Load the data and get quick sense of data](#Load-the-data-and-get-quick-sense-of-data)\n",
    "* [Exploring and analyzing data with DataFrames](#Exploring-and-analyzing-data-with-DataFrames)\n",
    "    - Transformations and actions on DataFrames\n",
    "    - Aggregation, grouping, sampling, ordering data with DataFrame\n",
    "    - Working with Joins in DataFrames + Using Broadcast Variables and Accumulators with DataFrame    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and get quick sense of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Read the compressed file content using Python's lzma library__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of file_content variable =  <class 'list'>\n",
      "OBJECTID,Identifier,Occurrence Date,Day of Week,Occurrence Month,Occurrence Day,Occurrence Year,Occurrence Hour,CompStat Month,CompStat Day,CompStat Year,Offense,Offense Classification,Sector,Precinct,Borough,Jurisdiction,XCoordinate,YCoordinate,Location 1\n",
      "\n",
      "1,f070032d,09/06/1940 07:30:00 PM,Friday,Sep,6,1940,19,9,7,2010,BURGLARY,FELONY,D,66,BROOKLYN,N.Y. POLICE DEPT,987478,166141,\"(40.6227027620001, -73.9883732929999)\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filePath=\"../02/NYPD_7_Major_Felony_Incidents.xz\"\n",
    "## For Linux \"file:///Users/tirthalp/something/gs-spark-python/notebooks/02/NYPD_7_Major_Felony_Incidents.xz\"\n",
    "## For Windows \"C:\\\\Users\\\\tirthalp\\\\something\\\\gs-spark-python\\\\notebooks\\\\02\\\\NYPD_7_Major_Felony_Incidents.xz\"\n",
    "\n",
    "import lzma\n",
    "with lzma.open(filePath, 'rt') as f:\n",
    "    file_content = list(f)\n",
    "\n",
    "print(\"Type of file_content variable = \", type(file_content))\n",
    "print(file_content[0])\n",
    "print(file_content[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert Python list into Spark's DataFrame__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .appName(\"Crime Analysis in New York City using Spark DataFrame\")\\\n",
    "                    .getOrCreate()\n",
    "\n",
    "### ---> Parse header and unstructured data, and prepare RDD of Crime objects\n",
    "\n",
    "data = spark.sparkContext.parallelize(file_content)\n",
    "\n",
    "# Clean '\\n' at the end of each line\n",
    "data = data.map(lambda x:x.replace(\"\\n\",\"\"))\n",
    "\n",
    "# Get the header row\n",
    "header = data.first()\n",
    "\n",
    "# Filter the header row\n",
    "dataWoHeader = data.filter(lambda x: x!=header)\n",
    "\n",
    "# How to transform records of string to named tuples / Parse the rows to extract fields?\n",
    "import csv\n",
    "from io import StringIO\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = header.replace(\" \",\"_\").replace(\"/\",\"_\").split(\",\")\n",
    "\n",
    "Crime = namedtuple('Crime', fields, verbose=False)\n",
    "\n",
    "def parse(row):\n",
    "    reader = csv.reader(StringIO(row))\n",
    "    row = next(reader)\n",
    "    return Crime(*row)\n",
    "\n",
    "# Transform String to Crime object\n",
    "crimesRdd=dataWoHeader.map(parse)\n",
    "\n",
    "type(crimesRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ---> RDD to DataFrame\n",
    "\n",
    "crimesDf = crimesRdd.toDF()\n",
    "\n",
    "type(crimesDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring and analyzing data with DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do data exploration and simple transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OBJECTID: string (nullable = true)\n",
      " |-- Identifier: string (nullable = true)\n",
      " |-- Occurrence_Date: string (nullable = true)\n",
      " |-- Day_of_Week: string (nullable = true)\n",
      " |-- Occurrence_Month: string (nullable = true)\n",
      " |-- Occurrence_Day: string (nullable = true)\n",
      " |-- Occurrence_Year: string (nullable = true)\n",
      " |-- Occurrence_Hour: string (nullable = true)\n",
      " |-- CompStat_Month: string (nullable = true)\n",
      " |-- CompStat_Day: string (nullable = true)\n",
      " |-- CompStat_Year: string (nullable = true)\n",
      " |-- Offense: string (nullable = true)\n",
      " |-- Offense_Classification: string (nullable = true)\n",
      " |-- Sector: string (nullable = true)\n",
      " |-- Precinct: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Jurisdiction: string (nullable = true)\n",
      " |-- XCoordinate: string (nullable = true)\n",
      " |-- YCoordinate: string (nullable = true)\n",
      " |-- Location_1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What's the schema?\n",
    "crimesDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+-----------+----------------+--------------+---------------+---------------+--------------+------------+-------------+-------------+----------------------+------+--------+---------+----------------+-----------+-----------+--------------------+\n",
      "|OBJECTID|Identifier|     Occurrence_Date|Day_of_Week|Occurrence_Month|Occurrence_Day|Occurrence_Year|Occurrence_Hour|CompStat_Month|CompStat_Day|CompStat_Year|      Offense|Offense_Classification|Sector|Precinct|  Borough|    Jurisdiction|XCoordinate|YCoordinate|          Location_1|\n",
      "+--------+----------+--------------------+-----------+----------------+--------------+---------------+---------------+--------------+------------+-------------+-------------+----------------------+------+--------+---------+----------------+-----------+-----------+--------------------+\n",
      "|       1|  f070032d|09/06/1940 07:30:...|     Friday|             Sep|             6|           1940|             19|             9|           7|         2010|     BURGLARY|                FELONY|     D|      66| BROOKLYN|N.Y. POLICE DEPT|     987478|     166141|(40.6227027620001...|\n",
      "|       2|  c6245d4d|12/14/1968 12:20:...|   Saturday|             Dec|            14|           1968|              0|            12|          14|         2008|GRAND LARCENY|                FELONY|     G|      28|MANHATTAN|N.Y. POLICE DEPT|     996470|     232106|(40.8037530600001...|\n",
      "|       3|  716dbc6f|10/30/1970 03:30:...|     Friday|             Oct|            30|           1970|             15|            10|          31|         2008|     BURGLARY|                FELONY|     H|      84| BROOKLYN|N.Y. POLICE DEPT|     986508|     190249|(40.688874254, -7...|\n",
      "+--------+----------+--------------------+-----------+----------------+--------------+---------------+---------------+--------------+------------+-------------+-------------+----------------------+------+--------+---------+----------------+-----------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore data by taking subset of DataFrame\n",
    "crimesDf.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------------+-------------+-------------+--------+---------+----------------+\n",
      "|OBJECTID|Identifier|Occurrence_Year|CompStat_Year|      Offense|Precinct|  Borough|    Jurisdiction|\n",
      "+--------+----------+---------------+-------------+-------------+--------+---------+----------------+\n",
      "|       1|  f070032d|           1940|         2010|     BURGLARY|      66| BROOKLYN|N.Y. POLICE DEPT|\n",
      "|       2|  c6245d4d|           1968|         2008|GRAND LARCENY|      28|MANHATTAN|N.Y. POLICE DEPT|\n",
      "|       3|  716dbc6f|           1970|         2008|     BURGLARY|      84| BROOKLYN|N.Y. POLICE DEPT|\n",
      "+--------+----------+---------------+-------------+-------------+--------+---------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use dropna to drop rows which have nas (i.e values that are not available)\n",
    "# crimesDf.dropna()\n",
    "\n",
    "# When doing huge analysis, for performance improvement, drop columns which are not required for the purpose of analysis\n",
    "crimesDf = crimesDf.drop(\"Occurrence_Date\", \"Day_of_Week\", \"Occurrence_Month\", \"Occurrence_Day\", \"Occurrence_Hour\", \"CompStat_Month\", \"CompStat_Day\", \"Offense_Classification\", \"Sector\", \"XCoordinate\", \"YCoordinate\", \"Location_1\")\n",
    "crimesDf.cache()\n",
    "\n",
    "crimesDf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             Offense|\n",
      "+--------------------+\n",
      "|      FELONY ASSAULT|\n",
      "|                  NA|\n",
      "|MURDER & NON-NEGL...|\n",
      "|             ROBBERY|\n",
      "|GRAND LARCENY OF ...|\n",
      "|                RAPE|\n",
      "|       GRAND LARCENY|\n",
      "|            BURGLARY|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How to see unique values on particular column?\n",
    "crimesDf.select('Offense').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|Offense                       |\n",
      "+------------------------------+\n",
      "|FELONY ASSAULT                |\n",
      "|MURDER & NON-NEGL. MANSLAUGHTE|\n",
      "|ROBBERY                       |\n",
      "|GRAND LARCENY OF MOTOR VEHICLE|\n",
      "|RAPE                          |\n",
      "|GRAND LARCENY                 |\n",
      "|BURGLARY                      |\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How to filter records with \"NA\" Offense type?\n",
    "crimesDf = crimesDf.filter(crimesDf['offense'] != \"NA\").distinct()\n",
    "crimesDf.select('Offense').distinct().show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1123464"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total records?\n",
    "crimesDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------------+-------------+--------+--------+--------+----------------+\n",
      "|OBJECTID|Identifier|Occurrence_Year|CompStat_Year| Offense|Precinct| Borough|    Jurisdiction|\n",
      "+--------+----------+---------------+-------------+--------+--------+--------+----------------+\n",
      "|    4830|  70ec8ef3|           2006|         2006|BURGLARY|      67|BROOKLYN|N.Y. POLICE DEPT|\n",
      "|   13444|  bd764603|           2006|         2006| ROBBERY|     107|  QUEENS|N.Y. POLICE DEPT|\n",
      "|   13578|  77c74c85|           2005|         2006|BURGLARY|      72|BROOKLYN|N.Y. POLICE DEPT|\n",
      "|   16834|  591e423e|           2006|         2006|BURGLARY|      90|BROOKLYN|N.Y. POLICE DEPT|\n",
      "|   35291|  7cd909b4|           2006|         2006|BURGLARY|      43|   BRONX|N.Y. POLICE DEPT|\n",
      "+--------+----------+---------------+-------------+--------+--------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How to see certain fraction of data for the particular types of offense using sample(fraction=n)?\n",
    "crimesDf.filter(crimesDf['offense'].isin([\"BURGLARY\", \"ROBBERY\"])).sample(fraction=0.1).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping, aggregation and ordering data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Year-on-Year Offenses growth pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|Occurrence_Year|offenses|\n",
      "+---------------+--------+\n",
      "|           2015|  102657|\n",
      "|           2014|  106849|\n",
      "|           2013|  111286|\n",
      "|           2012|  111798|\n",
      "|           2011|  107206|\n",
      "|           2010|  105643|\n",
      "|           2009|  106018|\n",
      "|           2008|  117375|\n",
      "|           2007|  120554|\n",
      "|           2006|  127887|\n",
      "|           2005|    3272|\n",
      "|           2004|     692|\n",
      "|           2003|     490|\n",
      "|           2002|     368|\n",
      "|           2001|     343|\n",
      "|           2000|     282|\n",
      "|           1999|     124|\n",
      "|           1998|      74|\n",
      "|           1997|      40|\n",
      "|           1996|      34|\n",
      "|           1995|      27|\n",
      "|           1994|      19|\n",
      "|           1993|      23|\n",
      "|           1992|      12|\n",
      "|           1991|      12|\n",
      "|           1990|      17|\n",
      "|           1989|      12|\n",
      "|           1988|       6|\n",
      "|           1987|       6|\n",
      "|           1986|       2|\n",
      "|           1985|       8|\n",
      "|           1984|       4|\n",
      "|           1983|       1|\n",
      "|           1982|       5|\n",
      "|           1981|       1|\n",
      "|           1980|       5|\n",
      "|           1979|       6|\n",
      "|           1978|       2|\n",
      "|           1977|       3|\n",
      "|           1976|       2|\n",
      "|           1975|       2|\n",
      "|           1974|       3|\n",
      "|           1973|       5|\n",
      "|           1972|       2|\n",
      "|           1971|       1|\n",
      "|           1970|       2|\n",
      "|           1969|       1|\n",
      "|           1968|       1|\n",
      "|           1966|       7|\n",
      "|           1965|       2|\n",
      "+---------------+--------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "crimesDf.groupBy(\"Occurrence_Year\").count().withColumnRenamed(\"count\", \"offenses\").orderBy(desc(\"Occurrence_Year\")).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1117273"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sudden increase in the number of offenses from Year 2006 onwards indicates anomany\n",
    "# So filtering data of occurrence year 2005 or prior\n",
    "\n",
    "crimesDf = crimesDf.filter(crimesDf[\"Occurrence_Year\"].isin([\"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\"]))\n",
    "\n",
    "crimesDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total number of offense by type? Which is highest number of offense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------+\n",
      "|Offense                       |count |\n",
      "+------------------------------+------+\n",
      "|GRAND LARCENY                 |424635|\n",
      "|ROBBERY                       |198569|\n",
      "|BURGLARY                      |191045|\n",
      "|FELONY ASSAULT                |183879|\n",
      "|GRAND LARCENY OF MOTOR VEHICLE|101728|\n",
      "|RAPE                          |12974 |\n",
      "|MURDER & NON-NEGL. MANSLAUGHTE|4443  |\n",
      "+------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crimesDf.groupBy('Offense').count().orderBy(desc('count')).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sum of precincts by Offense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|             Offense|  Precincts|\n",
      "+--------------------+-----------+\n",
      "|      FELONY ASSAULT|1.1921206E7|\n",
      "|MURDER & NON-NEGL...|   297252.0|\n",
      "|             ROBBERY|1.2970707E7|\n",
      "|GRAND LARCENY OF ...|  7586978.0|\n",
      "|                RAPE|   855586.0|\n",
      "|       GRAND LARCENY|2.3469435E7|\n",
      "|            BURGLARY|1.3247918E7|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# agg({\"Precinct\":\"sum\"}) = perform sum aggregation on 'Precinct' column using built-in agg function of Spark DF\n",
    "# withColumnRenamed = returns a new DataFrame by renaming an existing column\n",
    "\n",
    "crimesDf_offense_precinct_sum = crimesDf.filter(crimesDf[\"Precinct\"] != \"NA\")\\\n",
    "                                        .groupBy(\"Offense\")\\\n",
    "                                        .agg({\"Precinct\":\"sum\"})\\\n",
    "                                        .withColumnRenamed(\"sum(Precinct)\",\"Precincts\")\n",
    "\n",
    "crimesDf_offense_precinct_sum.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+---------+\n",
      "|Offense                       |Precincts|\n",
      "+------------------------------+---------+\n",
      "|BURGLARY                      |13247918 |\n",
      "|FELONY ASSAULT                |11921206 |\n",
      "|GRAND LARCENY                 |23469435 |\n",
      "|GRAND LARCENY OF MOTOR VEHICLE|7586978  |\n",
      "|MURDER & NON-NEGL. MANSLAUGHTE|297252   |\n",
      "|RAPE                          |855586   |\n",
      "|ROBBERY                       |12970707 |\n",
      "+------------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How to cast scientific notation in above (e.g. 2.3742835E7 value in Precincts column) to number?\n",
    "\n",
    "# withColumn = returns a new DataFrame by adding a column or replacing the existing column that has the same name\n",
    "\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "crimesDf_offense_precinct_sum = crimesDf_offense_precinct_sum\\\n",
    "                                        .withColumn('Precincts', crimesDf_offense_precinct_sum.Precincts.cast(DecimalType(18, 0)))\\\n",
    "                                        .orderBy(\"Offense\", desc(\"Precincts\"))\n",
    "\n",
    "crimesDf_offense_precinct_sum.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 3 Offense based on respective total incident occurrences and % incident?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             Offense|Incidents|\n",
      "+--------------------+---------+\n",
      "|      FELONY ASSAULT|   183879|\n",
      "|MURDER & NON-NEGL...|     4443|\n",
      "|             ROBBERY|   198569|\n",
      "|GRAND LARCENY OF ...|   101728|\n",
      "|                RAPE|    12974|\n",
      "|       GRAND LARCENY|   424635|\n",
      "|            BURGLARY|   191045|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Incidents by Offense?\n",
    "\n",
    "crimesDf_offense_incidents = crimesDf.groupBy(\"Offense\").count().withColumnRenamed(\"count\",\"Incidents\")\n",
    "\n",
    "crimesDf_offense_incidents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|sum(Incidents)|\n",
      "+--------------+\n",
      "|       1117273|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total number of incidents across all Offenses?\n",
    "\n",
    "crimesDf_offense_total = crimesDf_offense_incidents.agg({\"Incidents\": \"sum\"})\n",
    "\n",
    "crimesDf_offense_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1117273"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_offense_incidents = crimesDf_offense_total.collect()[0][0]\n",
    "\n",
    "total_offense_incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Offense: string (nullable = true)\n",
      " |-- Incidents: long (nullable = false)\n",
      " |-- % Incident: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Adding new column for % of Incident calculation\n",
    "crimesDf_offense_percentage_incident = crimesDf_offense_incidents.withColumn(\n",
    "    \"% Incident\",\n",
    "    round(crimesDf_offense_incidents.Incidents / total_offense_incidents * 100, 2))\n",
    "\n",
    "crimesDf_offense_percentage_incident.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----------+\n",
      "|      Offense|Incidents|% Incident|\n",
      "+-------------+---------+----------+\n",
      "|GRAND LARCENY|   424635|     38.01|\n",
      "|      ROBBERY|   198569|     17.77|\n",
      "|     BURGLARY|   191045|      17.1|\n",
      "+-------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Order by % Incident column to get top 3 offense with highest incidents\n",
    "\n",
    "crimesDf_offense_percentage_incident.orderBy(crimesDf_offense_percentage_incident[2].desc()).limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### More Aggregation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|max(Occurrence_Year)|sum(Precinct)|\n",
      "+--------------------+-------------+\n",
      "|                2015|  7.0349082E7|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiple aggregation operations can be applied\n",
    "crimesDf.agg({\"Precinct\" : \"sum\", \"Occurrence_Year\" : \"max\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|         Precinct|\n",
      "+-------+-----------------+\n",
      "|  count|          1117256|\n",
      "|   mean|62.96594692711429|\n",
      "| stddev|34.90093348021095|\n",
      "|    min|                1|\n",
      "|    max|               94|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uages of describe\n",
    "crimesDf.select(\"Precinct\").filter(crimesDf[\"Precinct\"] != \"NA\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to view information in matrix form using crosstab function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+-------+----+-------------+--------+\n",
      "|Borough_Offense|FELONY ASSAULT|ROBBERY|RAPE|GRAND LARCENY|BURGLARY|\n",
      "+---------------+--------------+-------+----+-------------+--------+\n",
      "|  STATEN ISLAND|          5505|   4442| 500|        11244|    6926|\n",
      "|         QUEENS|         35240|  40785|2975|        80704|   49648|\n",
      "|      MANHATTAN|         32979|  38478|2803|       165896|   35044|\n",
      "|       BROOKLYN|         62406|  69603|3850|       112922|   65841|\n",
      "|          BRONX|         47748|  45260|2846|        53867|   33584|\n",
      "+---------------+--------------+-------+----+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total occurrency of each Offense per Borough?\n",
    "\n",
    "crimesDf.filter(crimesDf[\"Borough\"] != \"\")\\\n",
    "        .filter(crimesDf[\"Borough\"] != \"(null)\")\\\n",
    "        .crosstab(\"Borough\", \"Offense\")\\\n",
    "        .select(\"Borough_Offense\", \"FELONY ASSAULT\", \"ROBBERY\", \"RAPE\", \"GRAND LARCENY\", \"BURGLARY\")\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Joins in DataFrames + Using Broadcast Variables and Accumulators with DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Broadcast Variables and Accumulators concepts in Spark\n",
    "\n",
    "* Spark is written in Scala, and heavily utilizes __closures__. (_Scala Closures are functions which uses one or more free variables and the return value of this function is dependent of these variable. The free variables are defined outside of the Closure Function and is not included as a parameter of this function. So the difference between a closure function and a normal function is the free variable. A free variable is any kind of variable which is not defined within the function and not passed as the parameter of the function. A free variable is not bound to a function with a valid value. The function does not contain any values for the free variable._). Also, the closure retains its copies of local variables - even after the outer scope ceases to exist.\n",
    "* In Spark 2.x architecture, Tasks which run on individual workers are closures. Every task will contain a copy of the variables that it works on for closures. That means, lots of copies are passed around from master to worker nodes for each tasks and shuffling would further cost hit. That's why need shared variables across tasks on individual worker nodes to solve this problem statement. In Spark, shared variables are: (1) Broadcast variables (2) Accumulator\n",
    "* __Broadcast Variables__:     \n",
    "    - Shared, read-only variables\n",
    "    - One copy per node (Not one copy per task)\n",
    "    - Distributed efficiently by Spark\n",
    "    - All nodes in cluster distribute (i.e. peer-to-peer copying too)\n",
    "    - No shuffling\n",
    "    - Will be cached in-memory on each node, so can be large, but not too large\n",
    "    - Use whenever tasks across stages need same data; Share dataset with all nodes like training data in ML, static lookup tables\n",
    "* __Accumulators__:    \n",
    "    - Read-write shared variables\n",
    "    - Added associatively { e.g. A + B = B + A } and communicatively { e.g. A + (B + C) = (A + B) + C } \n",
    "    - Spark native support for accumulators of type Long, Double and Collections; which can be extended by subclassing AccumulatorV2    \n",
    "    - Workers can only modify state\n",
    "    - Only the driver program can read state\n",
    "    - Use for counters or sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load another data set and get quick sense of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset 1: Offenses\n",
    "\n",
    "offensesDf = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Offense.csv\");\n",
    "\n",
    "type(offensesDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Offense: string (nullable = true)\n",
      " |-- Penalty_USD: string (nullable = true)\n",
      " |-- Punishment_Severity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "offensesDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----------+-------------------+\n",
      "|Offense                       |Penalty_USD|Punishment_Severity|\n",
      "+------------------------------+-----------+-------------------+\n",
      "|BURGLARY                      |100        |LOW                |\n",
      "|FELONY ASSAULT                |500        |MEDIUM             |\n",
      "|GRAND LARCENY                 |250        |LOW                |\n",
      "|GRAND LARCENY OF MOTOR VEHICLE|250        |LOW                |\n",
      "|MURDER & NON-NEGL. MANSLAUGHTE|1000       |HIGH               |\n",
      "|RAPE                          |750        |HIGH               |\n",
      "|ROBBERY                       |150        |LOW                |\n",
      "+------------------------------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "offensesDf.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Occurrence_Year: string (nullable = true)\n",
      " |-- Offense: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop unwanted columns\n",
    "\n",
    "crimesDf = crimesDf.drop(\"OBJECTID\", \"Identifier\", \"CompStat_Year\", \"Precinct\", \"Jurisdiction\")\n",
    "\n",
    "crimesDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------------+-------------+---------+\n",
      "|OBJECTID|Identifier|Occurrence_Year|Offense      |Borough  |\n",
      "+--------+----------+---------------+-------------+---------+\n",
      "|271     |ee300b6c  |2006           |GRAND LARCENY|BRONX    |\n",
      "|485     |9a3f0b4a  |2006           |GRAND LARCENY|MANHATTAN|\n",
      "|719     |f714294a  |2006           |GRAND LARCENY|MANHATTAN|\n",
      "|787     |77b2fd2d  |2006           |BURGLARY     |QUEENS   |\n",
      "|904     |5ec3445c  |2006           |GRAND LARCENY|QUEENS   |\n",
      "+--------+----------+---------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crimesDf.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 1117273)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offensesDf.count(), crimesDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Join two datasets  - Inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Occurrence_Year',\n",
       " 'Offense',\n",
       " 'Borough',\n",
       " 'Offense',\n",
       " 'Penalty_USD',\n",
       " 'Punishment_Severity']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime_details_temp = crimesDf.join(offensesDf, crimesDf.Offense == offensesDf.Offense)\n",
    "\n",
    "crime_details_temp.columns\n",
    "# Note: Offense column is two times, b'cas one from crimesDf and another from OffensesDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1117273"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime_details_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+---------+-------------+-----------+-------------------+\n",
      "|Occurrence_Year|      Offense|  Borough|      Offense|Penalty_USD|Punishment_Severity|\n",
      "+---------------+-------------+---------+-------------+-----------+-------------------+\n",
      "|           2006|GRAND LARCENY|    BRONX|GRAND LARCENY|        250|                LOW|\n",
      "|           2006|GRAND LARCENY|MANHATTAN|GRAND LARCENY|        250|                LOW|\n",
      "|           2006|GRAND LARCENY|MANHATTAN|GRAND LARCENY|        250|                LOW|\n",
      "+---------------+-------------+---------+-------------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crime_details_temp.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+---------+-----------+-------------------+\n",
      "|      Offense|Occurrence_Year|  Borough|Penalty_USD|Punishment_Severity|\n",
      "+-------------+---------------+---------+-----------+-------------------+\n",
      "|GRAND LARCENY|           2006|    BRONX|        250|                LOW|\n",
      "|GRAND LARCENY|           2006|MANHATTAN|        250|                LOW|\n",
      "|GRAND LARCENY|           2006|MANHATTAN|        250|                LOW|\n",
      "+-------------+---------------+---------+-----------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another syntax of joining two datasets, in which just providing column name to apply inner join\n",
    "\n",
    "# use \"how\" to apply join type = inner | left | right | full\n",
    "\n",
    "crimesDf.join(offensesDf, [\"Offense\"], how=\"inner\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Joins using Broadcast variables\n",
    "\n",
    "When performing joins on huge datasets, it would be heavy duty operation. In such case, it is recommeded to perform join operations by broadcasting DataFrames to share the data across tasks.\n",
    "\n",
    "Always broadcast the smaller DataFrame to all nodes. For example, offensesDf can be considered for broadcasting over crimesDf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1117273"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "crime_details = crimesDf.select(\"Offense\", \"Occurrence_Year\")\\\n",
    "                        .join(broadcast(offensesDf), [\"Offense\"], \"inner\")\n",
    "\n",
    "crime_details.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-----------+-------------------+\n",
      "|Offense|Occurrence_Year|Penalty_USD|Punishment_Severity|\n",
      "+-------+---------------+-----------+-------------------+\n",
      "|ROBBERY|           2006|        150|                LOW|\n",
      "|ROBBERY|           2006|        150|                LOW|\n",
      "|ROBBERY|           2006|        150|                LOW|\n",
      "|ROBBERY|           2006|        150|                LOW|\n",
      "|ROBBERY|           2006|        150|                LOW|\n",
      "+-------+---------------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crime_details.sort(crime_details.Offense.desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sum using Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Approach 1: Punishment Penalty sum by Severity using Accumulator\n",
    "\n",
    "# declare accumulator\n",
    "\n",
    "low_severity_penalty_sum = spark.sparkContext.accumulator(0.0)\n",
    "medium_severity_penalty_sum = spark.sparkContext.accumulator(0.0)\n",
    "high_severity_penalty_sum = spark.sparkContext.accumulator(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_penalty_by_severity(row):\n",
    "    severity = row.Punishment_Severity\n",
    "    penalty = float(row.Penalty_USD)\n",
    "\n",
    "# write / add to accumulator\n",
    "\n",
    "    if(severity == \"HIGH\"):\n",
    "        high_severity_penalty_sum.add(penalty)\n",
    "    elif(severity == \"MEDIUM\"):\n",
    "        medium_severity_penalty_sum.add(penalty)\n",
    "    elif(severity == \"LOW\"):\n",
    "        low_severity_penalty_sum.add(penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_details.foreach(lambda x: cal_penalty_by_severity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180480600.0, 91939500.0, 14173500.0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read accumulators\n",
    "\n",
    "all_penalty_sum_by_severity = (low_severity_penalty_sum.value, medium_severity_penalty_sum.value, high_severity_penalty_sum.value)\n",
    "\n",
    "all_penalty_sum_by_severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+\n",
      "|Punishment_Severity|Total_Penalty|\n",
      "+-------------------+-------------+\n",
      "|                LOW|  180480600.0|\n",
      "|             MEDIUM|   91939500.0|\n",
      "|               HIGH|   14173500.0|\n",
      "+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Approach 2: Punishment Penalty sum by Severity using inbuilt agg function of Spark DataFrame\n",
    "\n",
    "all_penalty_sum = crime_details.groupby(\"Punishment_Severity\")\\\n",
    "                               .agg({\"Penalty_USD\": \"sum\"})\\\n",
    "                               .withColumnRenamed(\"sum(Penalty_USD)\",\"Total_Penalty\")\n",
    "\n",
    "all_penalty_sum.withColumn('Total_Penalty', all_penalty_sum.Total_Penalty.cast(DecimalType(18, 1)))\\\n",
    "               .orderBy(desc(\"Total_Penalty\"))\\\n",
    "               .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
