{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Spark DataFrames and Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objective\n",
    "\n",
    "* [Creating DataFrames and performing DataFrame Operations (aka Untyped Dataset Operations)](#Creating-DataFrames-and-performing-DataFrame-Operations)\n",
    "* [Converting DataFrames to RDDs](#Converting-DataFrames-to-RDDs)\n",
    "* [Running SQL Queries Programmatically](#Running-SQL-Queries-Programmatically)\n",
    "* [Using SQL functions to perform calculations on data (e.g. built-in Scalar and Aggregate functions)](#Using-SQL-functions-to-perform-calculations-on-data)\n",
    "* [Table catalog and Hive metastore](#Table-catalog-and-Hive-metastore)\n",
    "* [Global Temporary View](#Global-Temporary-View)\n",
    "* [Saving and loading DataFrame data](#Saving-and-loading-DataFrame-data)\n",
    "\n",
    "At the end, see [Summary](#Summary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Point: SparkSession\n",
    "\n",
    "SparkSession in Spark 2.0 provides builtin support for Hive features including the ability to write queries using HiveQL, access to Hive UDFs, and the ability to read data from Hive tables. To use these features, you do not need to have an existing Hive setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Understanding Spark DataFrames and Spark SQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames and performing DataFrame Operations\n",
    "\n",
    "(aka Untyped Dataset Operations)\n",
    "\n",
    "With a SparkSession, applications can create DataFrames from an existing RDD, from a Hive table, or from Spark data sources.\n",
    "\n",
    "DataFrame’s DSL has a set of functionalities similar to the usual SQL functions for manipulating data in relational databases. DataFrames work like RDDs: they’re immutable and lazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filePath=\"persons.json\"\n",
    "## For Linux \"file:///Users/tirthalp/something/gs-spark-python/notebooks/11/persons.json\"\n",
    "## For Windows \"C:\\\\Users\\\\tirthalp\\\\something\\\\gs-spark-python\\\\notebooks\\\\11\\\\persons.json\"\n",
    "\n",
    "df = spark.read.option(\"multiLine\", \"true\") \\\n",
    "    .json(filePath)\n",
    "\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting schema information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- about: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- balance: string (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- favoriteFruit: string (nullable = true)\n",
      " |-- friends: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- greeting: string (nullable = true)\n",
      " |-- guid: string (nullable = true)\n",
      " |-- index: long (nullable = true)\n",
      " |-- isActive: boolean (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- picture: string (nullable = true)\n",
      " |-- registered: string (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema in a tree format\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id',\n",
       " 'about',\n",
       " 'address',\n",
       " 'age',\n",
       " 'balance',\n",
       " 'company',\n",
       " 'email',\n",
       " 'eyeColor',\n",
       " 'favoriteFruit',\n",
       " 'friends',\n",
       " 'gender',\n",
       " 'greeting',\n",
       " 'guid',\n",
       " 'index',\n",
       " 'isActive',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'name',\n",
       " 'phone',\n",
       " 'picture',\n",
       " 'registered',\n",
       " 'tags']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_id', 'string'),\n",
       " ('about', 'string'),\n",
       " ('address', 'string'),\n",
       " ('age', 'bigint'),\n",
       " ('balance', 'string'),\n",
       " ('company', 'string'),\n",
       " ('email', 'string'),\n",
       " ('eyeColor', 'string'),\n",
       " ('favoriteFruit', 'string'),\n",
       " ('friends', 'array<struct<id:bigint,name:string>>'),\n",
       " ('gender', 'string'),\n",
       " ('greeting', 'string'),\n",
       " ('guid', 'string'),\n",
       " ('index', 'bigint'),\n",
       " ('isActive', 'boolean'),\n",
       " ('latitude', 'double'),\n",
       " ('longitude', 'double'),\n",
       " ('name', 'string'),\n",
       " ('phone', 'string'),\n",
       " ('picture', 'string'),\n",
       " ('registered', 'string'),\n",
       " ('tags', 'array<string>')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---+---------+--------+--------------------+--------+-------------+--------------------+------+--------------------+--------------------+-----+--------+----------+----------+----------+-----------------+--------------------+--------------------+--------------------+\n",
      "|                 _id|               about|             address|age|  balance| company|               email|eyeColor|favoriteFruit|             friends|gender|            greeting|                guid|index|isActive|  latitude| longitude|      name|            phone|             picture|          registered|                tags|\n",
      "+--------------------+--------------------+--------------------+---+---------+--------+--------------------+--------+-------------+--------------------+------+--------------------+--------------------+-----+--------+----------+----------+----------+-----------------+--------------------+--------------------+--------------------+\n",
      "|5c6158a17fd27b5a4...|Reprehenderit aut...|652 McDonald Aven...| 39|$2,868.75|BUZZNESS|popesoto@buzzness...|    blue|        apple|[[0, Norman Osbor...|  male|Hello, Pope Soto!...|aa247d9e-6562-4e4...|    0|    true|-56.739659|  57.73213| Pope Soto|+1 (911) 439-2209|http://placehold....|2017-04-26T11:55:...|[consectetur, lab...|\n",
      "|5c6158a1c3a640b52...|Duis ea ipsum iru...|826 Auburn Place,...| 23|$2,050.70| FORTEAN|lilyolson@fortean...|   green|        apple|[[0, Dillon Richa...|female|Hello, Lily Olson...|6af17eaf-abe9-4b9...|    1|    true|-57.474418|-91.740489|Lily Olson|+1 (830) 580-2068|http://placehold....|2017-10-20T09:06:...|[exercitation, ad...|\n",
      "+--------------------+--------------------+--------------------+---+---------+--------+--------------------+--------+-------------+--------------------+------+--------------------+--------------------+-----+--------+----------+----------+----------+-----------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displays the content of the DataFrame to stdout in a tabular form\n",
    "\n",
    "# By default, truncates long strings (i.e. strings more than 20 characters will be truncated and all cells will be aligned right)\n",
    "df.show(2) # Display two records\n",
    "\n",
    "# False = display complete strings without truncating\n",
    "# df.show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------+\n",
      "|_id                     |name         |\n",
      "+------------------------+-------------+\n",
      "|5c6158a17fd27b5a496f1383|Pope Soto    |\n",
      "|5c6158a1c3a640b523a977ee|Lily Olson   |\n",
      "|5c6158a1cd7259ec33888730|Elisa Burgess|\n",
      "+------------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the \"_id\" and \"name\" columns\n",
    "\n",
    "df.select(\"_id\", \"name\").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+---------+\n",
      "|          name|age|(age + 5)|\n",
      "+--------------+---+---------+\n",
      "|     Pope Soto| 39|       44|\n",
      "|    Lily Olson| 23|       28|\n",
      "| Elisa Burgess| 31|       36|\n",
      "| Beasley Bryan| 37|       42|\n",
      "|    Patti Barr| 23|       28|\n",
      "|Betsy Callahan| 35|       40|\n",
      "+--------------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select everybody, but increment the age by 5\n",
    "\n",
    "df.select(df['name'], df['age'], df['age'] + 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---+---------+--------+--------------------+--------+-------------+--------------------+------+--------------------+--------------------+-----+--------+----------+---------+-------------+-----------------+--------------------+--------------------+--------------------+\n",
      "|                 _id|               about|             address|age|  balance| company|               email|eyeColor|favoriteFruit|             friends|gender|            greeting|                guid|index|isActive|  latitude|longitude|         name|            phone|             picture|          registered|                tags|\n",
      "+--------------------+--------------------+--------------------+---+---------+--------+--------------------+--------+-------------+--------------------+------+--------------------+--------------------+-----+--------+----------+---------+-------------+-----------------+--------------------+--------------------+--------------------+\n",
      "|5c6158a17fd27b5a4...|Reprehenderit aut...|652 McDonald Aven...| 39|$2,868.75|BUZZNESS|popesoto@buzzness...|    blue|        apple|[[0, Norman Osbor...|  male|Hello, Pope Soto!...|aa247d9e-6562-4e4...|    0|    true|-56.739659| 57.73213|    Pope Soto|+1 (911) 439-2209|http://placehold....|2017-04-26T11:55:...|[consectetur, lab...|\n",
      "|5c6158a15e2a20452...|Officia nulla ea ...|686 Havemeyer Str...| 37|$2,320.67|   ZIPAK|beasleybryan@zipa...|   brown|   strawberry|[[0, Jenkins Reye...|  male|Hello, Beasley Br...|d170ae36-711d-477...|    3|   false|  8.883741|-3.459615|Beasley Bryan|+1 (866) 535-2803|http://placehold....|2018-03-03T07:31:...|[elit, irure, ea,...|\n",
      "+--------------------+--------------------+--------------------+---+---------+--------+--------------------+--------+-------------+--------------------+------+--------------------+--------------------+-----+--------+----------+---------+-------------+-----------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter data - Select people older than 35\n",
    "\n",
    "df.filter(df['age'] > 35).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 31|    1|\n",
      "| 39|    1|\n",
      "| 37|    1|\n",
      "| 35|    1|\n",
      "| 23|    2|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count people by age\n",
    "\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+---------+--------+--------------------+--------+-------------+--------------------+------+--------------------+--------------------+-----+--------+----------+---------+---------+-----------------+--------------------+--------------------+--------------------+\n",
      "|                 _id|             address|age|  balance| company|               email|eyeColor|favoriteFruit|             friends|gender|            greeting|                guid|index|isActive|  latitude|longitude|     name|            phone|             picture|          registered|                tags|\n",
      "+--------------------+--------------------+---+---------+--------+--------------------+--------+-------------+--------------------+------+--------------------+--------------------+-----+--------+----------+---------+---------+-----------------+--------------------+--------------------+--------------------+\n",
      "|5c6158a17fd27b5a4...|652 McDonald Aven...| 39|$2,868.75|BUZZNESS|popesoto@buzzness...|    blue|        apple|[[0, Norman Osbor...|  male|Hello, Pope Soto!...|aa247d9e-6562-4e4...|    0|    true|-56.739659| 57.73213|Pope Soto|+1 (911) 439-2209|http://placehold....|2017-04-26T11:55:...|[consectetur, lab...|\n",
      "+--------------------+--------------------+---+---------+--------+--------------------+--------+-------------+--------------------+------+--------------------+--------------------+-----+--------+----------+---------+---------+-----------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove given column\n",
    "\n",
    "df.drop(\"about\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+---+\n",
      "|                 _id|          name|age|\n",
      "+--------------------+--------------+---+\n",
      "|5c6158a1d8e19480b...|    Patti Barr| 23|\n",
      "|5c6158a1c3a640b52...|    Lily Olson| 23|\n",
      "|5c6158a1cd7259ec3...| Elisa Burgess| 31|\n",
      "|5c6158a10c3e6ca29...|Betsy Callahan| 35|\n",
      "|5c6158a15e2a20452...| Beasley Bryan| 37|\n",
      "|5c6158a17fd27b5a4...|     Pope Soto| 39|\n",
      "+--------------------+--------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sorting\n",
    "\n",
    "df.select(\"_id\", \"name\", \"age\").orderBy(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performing joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When having related data in two DataFrames and want the resulting DataFrame to contain rows from both DataFrames with values common to both, then join can be performed.\n",
    "\n",
    "Supported join types = inner, outer, left_outer, right_outer, or leftsemi.\n",
    "\n",
    "For example,\n",
    "\n",
    "val innerDataframe = someDf1.join(someDf2, someDf1(\"id\") === 'uid)\n",
    "\n",
    "The above performs an inner join. The outer join can be can performed by adding another argument.\n",
    "\n",
    "val outerDataframe = someDf1.join(someDf2, someDf1(\"id\") === 'uid, \"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Working with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose to drop the rows containing null or NaN (the Scala constant meaning “not a number”) values\n",
    "\n",
    "cleanDf = df.na.drop()\n",
    "cleanDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping column names to replacement values,e.g. replace null values in the age column with zeroes\n",
    "\n",
    "# Use -- df.na.fill(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting DataFrames to RDDs\n",
    "\n",
    "Every DataFrame has the lazily evaluated rdd field for accessing the underlying RDD. \n",
    "\n",
    "The resulting RDD contains elements of type org.apache.spark.sql.Row. Row has various get* functions for accessing column values\n",
    "by column indexes (getString(index), getInt(index), getMap(index), and so forth. It also has a useful function for converting rows to strings (mimicking similar function available for Scala sequences): mkString(delimiter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_id='5c6158a17fd27b5a496f1383', about='Reprehenderit aute ullamco occaecat cillum ex ex ex consectetur consequat. Ut cupidatat commodo anim incididunt aliquip qui ex. Deserunt sint laboris quis commodo ut reprehenderit nulla. Laborum nisi veniam anim proident laborum sint est ex id veniam nostrud. Deserunt id enim sit ipsum duis deserunt irure. Sint eiusmod elit aliquip ex aliqua pariatur.\\r\\n', address='652 McDonald Avenue, Sunwest, Montana, 4757', age=39, balance='$2,868.75', company='BUZZNESS', email='popesoto@buzzness.com', eyeColor='blue', favoriteFruit='apple', friends=[Row(id=0, name='Norman Osborne'), Row(id=1, name='Bauer Martinez'), Row(id=2, name='Zimmerman Lancaster')], gender='male', greeting='Hello, Pope Soto! You have 3 unread messages.', guid='aa247d9e-6562-4e42-8f36-e95e98ea9e84', index=0, isActive=True, latitude=-56.739659, longitude=57.73213, name='Pope Soto', phone='+1 (911) 439-2209', picture='http://placehold.it/32x32', registered='2017-04-26T11:55:39 -06:-30', tags=['consectetur', 'labore', 'laborum', 'ullamco', 'sint', 'laboris', 'anim']),\n",
       " Row(_id='5c6158a1c3a640b523a977ee', about='Duis ea ipsum irure Lorem ipsum ut fugiat et laborum anim do consequat. Excepteur non non anim sint magna duis proident. Proident do ea do Lorem.\\r\\n', address='826 Auburn Place, Whipholt, South Dakota, 3165', age=23, balance='$2,050.70', company='FORTEAN', email='lilyolson@fortean.com', eyeColor='green', favoriteFruit='apple', friends=[Row(id=0, name='Dillon Richard'), Row(id=1, name='Shawn Ramos'), Row(id=2, name='Ivy Jones')], gender='female', greeting='Hello, Lily Olson! You have 9 unread messages.', guid='6af17eaf-abe9-4b97-a627-890813cf775d', index=1, isActive=True, latitude=-57.474418, longitude=-91.740489, name='Lily Olson', phone='+1 (830) 580-2068', picture='http://placehold.it/32x32', registered='2017-10-20T09:06:49 -06:-30', tags=['exercitation', 'adipisicing', 'proident', 'consequat', 'in', 'culpa'])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = df.rdd\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SQL Queries Programmatically\n",
    "\n",
    "* Spark SQL enables quering of DataFrames as database tables - The DataFrame DSL functionalities are accessible through SQL commands as an alternative interface for programming Spark SQL. When you write SQL commands in Spark SQL, they get translated into operations on DataFrames. \n",
    "\n",
    "* Temporary per-session and global tables.\n",
    "\n",
    "* __The Catalyst optimizer__ of Spark makes SQL queries faster, which is the optimization engine that powers Spark SQL (as well as DataFrame API) since 2015. For more detail, [see](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html).\n",
    "\n",
    "* SQL statements can be run by using the sql methods provided by spark.\n",
    "\n",
    "* Tables schema can be inferred or explicitly specified\n",
    "\n",
    "* Advanced window operations are also supported\n",
    "\n",
    "* When using a SparkSession with Hive support, which is the recommended Spark SQL engine, the majority of Hive commands and data types are supported. Spark supports two SQL dialects: Spark’s SQL dialect and Hive Query Language (HQL). The Spark community recommends HQL (Spark 1.5) because HQL has a richer set of functionalities.\n",
    "\n",
    "* Spark also lets you run SQL commands remotely, through a JDBC (and ODBC) server called __Thrift__. For more detail, [see](https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html).\n",
    "\n",
    "* In addition to the Spark shell, Spark also offers an SQL shell in the form of the __spark-sql__ CLI, which supports the same arguments as the spark-shell and spark-submit commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---+---------+--------+--------------------+--------+-------------+--------------------+------+--------------------+--------------------+-----+--------+----------+----------+-------------+-----------------+--------------------+--------------------+--------------------+\n",
      "|                 _id|               about|             address|age|  balance| company|               email|eyeColor|favoriteFruit|             friends|gender|            greeting|                guid|index|isActive|  latitude| longitude|         name|            phone|             picture|          registered|                tags|\n",
      "+--------------------+--------------------+--------------------+---+---------+--------+--------------------+--------+-------------+--------------------+------+--------------------+--------------------+-----+--------+----------+----------+-------------+-----------------+--------------------+--------------------+--------------------+\n",
      "|5c6158a17fd27b5a4...|Reprehenderit aut...|652 McDonald Aven...| 39|$2,868.75|BUZZNESS|popesoto@buzzness...|    blue|        apple|[[0, Norman Osbor...|  male|Hello, Pope Soto!...|aa247d9e-6562-4e4...|    0|    true|-56.739659|  57.73213|    Pope Soto|+1 (911) 439-2209|http://placehold....|2017-04-26T11:55:...|[consectetur, lab...|\n",
      "|5c6158a1c3a640b52...|Duis ea ipsum iru...|826 Auburn Place,...| 23|$2,050.70| FORTEAN|lilyolson@fortean...|   green|        apple|[[0, Dillon Richa...|female|Hello, Lily Olson...|6af17eaf-abe9-4b9...|    1|    true|-57.474418|-91.740489|   Lily Olson|+1 (830) 580-2068|http://placehold....|2017-10-20T09:06:...|[exercitation, ad...|\n",
      "|5c6158a1cd7259ec3...|Velit ea fugiat c...|280 Mill Lane, Be...| 31|$1,692.66|  SQUISH|elisaburgess@squi...|   brown|   strawberry|[[0, Skinner Camp...|female|Hello, Elisa Burg...|708609a6-584f-4be...|    2|   false| 20.000327| 88.523831|Elisa Burgess|+1 (892) 444-2914|http://placehold....|2018-07-01T05:58:...|[exercitation, cu...|\n",
      "+--------------------+--------------------+--------------------+---+---------+--------+--------------------+--------+-------------+--------------------+------+--------------------+--------------------+-----+--------+----------+----------+-------------+-----------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"persons\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM persons\")\n",
    "sqlDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+---+\n",
      "|                 _id|         name|age|\n",
      "+--------------------+-------------+---+\n",
      "|5c6158a17fd27b5a4...|    Pope Soto| 39|\n",
      "|5c6158a15e2a20452...|Beasley Bryan| 37|\n",
      "+--------------------+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find male persons\n",
    "\n",
    "spark.sql(\"SELECT _id, name, age FROM persons WHERE gender = 'male'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SQL functions to perform calculations on data\n",
    "\n",
    "SQL functions fit into four categories:\n",
    "* __Scalar functions__ return a single value for each row based on calculations on one or more columns.\n",
    "* __Aggregate functions__ return a single value for a group of rows.\n",
    "* __Window functions__ return several values for a group of rows.\n",
    "* __User-defined functions__ include custom scalar or aggregate functions.\n",
    "\n",
    "See [Spark SQL, Built-in functions](https://spark.apache.org/docs/latest/api/sql/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Scalar functions\n",
    "\n",
    "The built-in scalar functions = Math calculations, String operations and Date-time operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------+\n",
      "|                 _id|          name|    name_ucase|\n",
      "+--------------------+--------------+--------------+\n",
      "|5c6158a17fd27b5a4...|     Pope Soto|     POPE SOTO|\n",
      "|5c6158a1c3a640b52...|    Lily Olson|    LILY OLSON|\n",
      "|5c6158a1cd7259ec3...| Elisa Burgess| ELISA BURGESS|\n",
      "|5c6158a15e2a20452...| Beasley Bryan| BEASLEY BRYAN|\n",
      "|5c6158a1d8e19480b...|    Patti Barr|    PATTI BARR|\n",
      "|5c6158a10c3e6ca29...|Betsy Callahan|BETSY CALLAHAN|\n",
      "+--------------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find average age of all males?\n",
    "\n",
    "spark.sql(\"SELECT _id, name, UPPER(name) as name_ucase FROM persons\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Aggregation functions\n",
    "\n",
    "The [built-in DataFrames functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$) provide common aggregations such as count(), countDistinct(), avg(), max(), min(), etc. While those functions are designed for DataFrames, Spark SQL also has type-safe versions for some of them in Scala and Java to work with strongly typed Datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|avg_male_age|\n",
      "+------------+\n",
      "|        38.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find average age of all males?\n",
    "\n",
    "spark.sql(\"SELECT AVG(age) as avg_male_age FROM persons WHERE gender = 'male'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|min_female_age|\n",
      "+--------------+\n",
      "|            23|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the youngest female age?\n",
    "\n",
    "spark.sql(\"SELECT MIN(age) AS min_female_age FROM persons WHERE gender = 'female'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Window functions\n",
    "\n",
    "Window functions = which operate over a range of rows in a DataFrame\n",
    "\n",
    "Before 1.4, there were two kinds of functions supported by Spark SQL that could be used to calculate a single return value. Built-in functions or UDFs, such as substr or round, take values from a single row as input, and they generate a single return value for every input row. Aggregate functions, such as SUM or MAX, operate on a group of rows and calculate a single return value for every group.\n",
    "\n",
    "While these are both very useful in practice, there is still a wide range of operations that cannot be expressed using these types of functions alone. Specifically, there was no way to both operate on a group of rows while still returning a single value for every input row. This limitation makes it hard to conduct various data processing tasks like calculating a moving average, calculating a cumulative sum, or accessing the values of a row appearing before the current row. Fortunately for users of Spark SQL, window functions fill this gap.\n",
    "\n",
    "Window functions can be used to answer questions like: \"Find the top selling mobile phone in last week's sell by performing operations\". For this key considerations would be: Partition by = home product categories; Order by = no sold; Frame = one week; \n",
    "\n",
    "Ranking and analytic functions (e.g. first, last, lag, lead, ntile, cumeDist, rank, denseRank, percentRank, RowNumber) can be used as window functions - [See](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+---+----+\n",
      "|          name|gender|age|rank|\n",
      "+--------------+------+---+----+\n",
      "|Betsy Callahan|female| 35|   1|\n",
      "| Elisa Burgess|female| 31|   2|\n",
      "|     Pope Soto|  male| 39|   1|\n",
      "| Beasley Bryan|  male| 37|   2|\n",
      "+--------------+------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find top two oldest persons in each gender category (e.g. male and female)?\n",
    "\n",
    "# For this - need to rank person in a category based on their age, and to pick the person based the ranking. \n",
    "# Below is the SQL query used to answer this question by using window function dense_rank\n",
    "\n",
    "spark.sql(\"SELECT name, gender, age, rank \\\n",
    "FROM ( \\\n",
    "  SELECT name, gender, age, DENSE_RANK() OVER (PARTITION BY gender ORDER BY age DESC) as rank \\\n",
    "  FROM persons) tmp \\\n",
    "WHERE \\\n",
    "  rank <= 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+---+--------------+\n",
      "|          name|gender|age|age_difference|\n",
      "+--------------+------+---+--------------+\n",
      "|Betsy Callahan|female| 35|             0|\n",
      "| Elisa Burgess|female| 31|             4|\n",
      "|    Lily Olson|female| 23|            12|\n",
      "|    Patti Barr|female| 23|            12|\n",
      "|     Pope Soto|  male| 39|             0|\n",
      "| Beasley Bryan|  male| 37|             2|\n",
      "+--------------+------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the difference between the age of each person and the most-oldest person in the respective gender category (male/female)?\n",
    "\n",
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "windowSpec = Window \\\n",
    "    .partitionBy(df['gender']) \\\n",
    "    .orderBy(df['age'].desc()) \\\n",
    "    .rangeBetween(-sys.maxsize, sys.maxsize)\n",
    "\n",
    "dataFrame = sqlContext.table(\"persons\")\n",
    "\n",
    "age_difference = (func.max(dataFrame['age']).over(windowSpec) - dataFrame['age'])\n",
    "\n",
    "dataFrame.select(dataFrame['name'], dataFrame['gender'], dataFrame['age'], age_difference.alias(\"age_difference\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-defined functions\n",
    "\n",
    "In many situations, Spark SQL may not provide the specific functionality you need in a particular moment. UDFs let you extend the built-in functionalities of Spark SQL.\n",
    "\n",
    "In short, Spark users are not limited to use the predefined or built-in functions only, rather can create their own user-defined functions too.\n",
    "\n",
    "[See](https://docs.databricks.com/spark/latest/spark-sql/udf-python.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+----------+\n",
      "|          name|                tags|tags_count|\n",
      "+--------------+--------------------+----------+\n",
      "|    Patti Barr|[minim, duis, ali...|         4|\n",
      "|    Lily Olson|[exercitation, ad...|         6|\n",
      "|     Pope Soto|[consectetur, lab...|         7|\n",
      "| Elisa Burgess|[exercitation, cu...|         7|\n",
      "|Betsy Callahan|[cupidatat, conse...|         7|\n",
      "| Beasley Bryan|[elit, irure, ea,...|         7|\n",
      "+--------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How to define custom UDF in SQL, for example to count tags?\n",
    "\n",
    "# Register a function as a UDF\n",
    "def count_tags(tags):\n",
    "    return len(tags)\n",
    "\n",
    "spark.udf.register(\"UDF_COUNT_TAGS\", count_tags)\n",
    "\n",
    "# Call the UDF in Spark SQL\n",
    "spark.sql(\"SELECT name, tags, UDF_COUNT_TAGS(tags) AS tags_count FROM persons ORDER BY tags_count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table catalog and Hive metastore\n",
    "\n",
    "When executing SQL queries using Spark SQL, you can reference a DataFrame by its name by registering the DataFrame as a table. When you do that, Spark stores the table definition in the table catalog.\n",
    "\n",
    "Hive functionalities can be enabled in Spark by calling enableHiveSupport() on a Builder object while constructing a SparkSession.\n",
    "\n",
    "    val spark = SparkSession.builder().enableHiveSupport().getOrCreate()\n",
    "\n",
    "__For Spark without Hive support__, a table catalog is implemented as a simple in-memory map, which means that table information lives in the driver’s memory and disappears with the Spark session. \n",
    "\n",
    "__SparkSession with Hive support__, on the other hand, uses a Hive metastore for implementing the table catalog. A Hive metastore is a persistent database, so DataFrame definitions remain available even if you close the Spark session and start a new one.\n",
    "\n",
    "__Registering tables temporarily__: Hive support still enables you to create temporary table definitions. In both cases (Spark with or without Hive support), the createOrReplaceTempView method registers a temporary table.\n",
    "\n",
    "__Registering tables permanetly__: Only SparkSession with Hive support can be used to register table definitions that will survive your application’s restarts (in other words, they’re persistent). By default, HiveContext creates a Derby database in the local working directory under the metastore_db subdirectory (or it reuses the database if it already exists). If you wish to change where the working directory is located, set the \"hive.metastore.warehouse.dir\" property in your \"hive-site.xml\" fileTo register a DataFrame as a permanent table, you need to use its write member.\n",
    "\n",
    "__Working with the Spark table catalog__: Since version 2.0, Spark provides a facility for managing the table catalog. To see which tables are currently registered, use: \n",
    "\n",
    "    spark.catalog.listTables().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Temporary View\n",
    "\n",
    "Register the table as Global Temporary View to make it accessible across all Spark sessions.\n",
    "\n",
    "__Temporary views__ in Spark SQL are session-scoped and will disappear if the session that creates it terminates. If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a __global temporary view__. \n",
    "\n",
    "Global temporary view is tied to a system preserved database __global_temp__, and we must use the qualified name to refer it, e.g. SELECT * FROM global_temp.view1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a global temporary view\n",
    "df.createGlobalTempView(\"persons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+\n",
      "|                 _id|          name|             friends|\n",
      "+--------------------+--------------+--------------------+\n",
      "|5c6158a17fd27b5a4...|     Pope Soto|[Norman Osborne, ...|\n",
      "|5c6158a1c3a640b52...|    Lily Olson|[Dillon Richard, ...|\n",
      "|5c6158a1cd7259ec3...| Elisa Burgess|[Skinner Campbell...|\n",
      "|5c6158a15e2a20452...| Beasley Bryan|[Jenkins Reyes, C...|\n",
      "|5c6158a1d8e19480b...|    Patti Barr|[Aline Griffin, L...|\n",
      "|5c6158a10c3e6ca29...|Betsy Callahan|[Langley Valdez, ...|\n",
      "+--------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT _id, name, friends.name as friends FROM global_temp.persons\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+\n",
      "|                 _id|          name|             friends|\n",
      "+--------------------+--------------+--------------------+\n",
      "|5c6158a17fd27b5a4...|     Pope Soto|[Norman Osborne, ...|\n",
      "|5c6158a1c3a640b52...|    Lily Olson|[Dillon Richard, ...|\n",
      "|5c6158a1cd7259ec3...| Elisa Burgess|[Skinner Campbell...|\n",
      "|5c6158a15e2a20452...| Beasley Bryan|[Jenkins Reyes, C...|\n",
      "|5c6158a1d8e19480b...|    Patti Barr|[Aline Griffin, L...|\n",
      "|5c6158a10c3e6ca29...|Betsy Callahan|[Langley Valdez, ...|\n",
      "+--------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Global temporary view is cross-session\n",
    "spark.newSession().sql(\"SELECT _id, name, friends.name as friends FROM global_temp.persons\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading DataFrame data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spark has built-in support for several file formats and databases (generally called data sources in Spark). These include JDBC and Hive, and the JSON, ORC, and Parquet file formats. \n",
    "* For relational databases, Spark specifically supports dialects (meaning data-type mappings) for the MySQL and PostgreSQL databases.\n",
    "* Data sources are pluggable, so you can add your own implementations. And you can download and use some external data, such as the [CSV](https://github.com/databricks/spark-csv), [Avro](https://github.com/databricks/spark-avro), and [Amazon Redshift](https://github.com/databricks/spark-redshift) data sources.\n",
    "* Spark uses the metastore, which we covered in the previous section, to save information about where and how the data is stored. It uses data sources for saving and loading the actual data.\n",
    "\n",
    "__Built-in data sources__\n",
    "\n",
    "* __JSON__: Commonly used for web development and is popular as a lightweight alternative to XML. Simple, easy to use, and human-readable. However, not an efficient permanent data-storage format.\n",
    "* __ORC__: Optimized row columnar (ORC) file format. Efficient way to store Hive data. Data from a single column is physically stored in close proximity.\n",
    "* __Parquet__: Started outside Hive and was later integrated with it. Designed to be independent of any specific framework and free of unnecessary dependencies. More popular in the Hadoop ecosystem than the file format ORC. Columnar file format and also uses compression. Parquet is the default data source in Spark.\n",
    "\n",
    "__Saving data__\n",
    "    \n",
    "    df.write.format(\"orc\").mode(\"overwrite\").option(...).saveAsTable(\"persons\")\n",
    "\n",
    "* DataFrame’s data is saved using the DataFrameWriter object, available as DataFrame’s write field. \n",
    "* In addition to the saveAsTable method, data can be saved using the save and insertInto methods. saveAsTable and insertInto save data into Hive tables and use the metastore in the process; save doesn’t. In case you’re not using Spark session with Hive support, the saveAsTable and insertInto methods create (or insert into) temporary tables. \n",
    "* You can configure all three methods with DataFrameWriter’s configuration functions.\n",
    "    - \"format\": Specifies the file format for saving data (the data source name), which can be one of the built-in data sources (json, parquet, orc) or a named custom data source. When no format is specified, the default is parquet.\n",
    "    - \"mode\": Specifies the save mode when a table or a file already exists. Possible values are overwrite (overwrites the existing data), append (appends the data), ignore (does nothing), and error (throws an exception); the default is error.\n",
    "    - \"option and options\": Adds a single parameter name and a value (or a parametervalue map) to the data source configuration.\n",
    "    - \"partitionBy\"\" Specifies partitioning columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.newSession().sql(\"SELECT _id, name, friends.name as friends FROM global_temp.persons\") \\\n",
    "                  .write.format(\"json\") \\\n",
    "                  .save(\"out\\\\persons-friends.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__About usage of coalesce()__\n",
    "\n",
    "* In above code, coalesce() was not used. So the number of files written out would be equal to the number of partitions in the DataFrame.\n",
    "\n",
    "* If coalesce() is used, then Spark would repartition the DataFrame into a single partition to write out a single file. For example,\n",
    "    ```\n",
    "    spark.newSession().sql(\"SELECT _id, name, friends.name as friends FROM global_temp.persons\") \\\n",
    "                      .coalesce(1) \\\n",
    "                      .write.format(\"json\") \\\n",
    "                      .save(\"out\\\\persons-friends.json\")\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save in parquet and load it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result in parquet format\n",
    "\n",
    "spark.newSession().sql(\"SELECT _id, name, friends.name as friends FROM global_temp.persons\") \\\n",
    "    .write.format(\"parquet\") \\\n",
    "    .save(\"out\\\\persons-friends.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+\n",
      "|                 _id|          name|             friends|\n",
      "+--------------------+--------------+--------------------+\n",
      "|5c6158a17fd27b5a4...|     Pope Soto|[Norman Osborne, ...|\n",
      "|5c6158a1c3a640b52...|    Lily Olson|[Dillon Richard, ...|\n",
      "|5c6158a1cd7259ec3...| Elisa Burgess|[Skinner Campbell...|\n",
      "|5c6158a15e2a20452...| Beasley Bryan|[Jenkins Reyes, C...|\n",
      "|5c6158a1d8e19480b...|    Patti Barr|[Aline Griffin, L...|\n",
      "|5c6158a10c3e6ca29...|Betsy Callahan|[Langley Valdez, ...|\n",
      "+--------------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the data from parquet format\n",
    "\n",
    "spark.read.parquet(\"out\\\\persons-friends.parquet\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to RDBMS and load it using JDBC\n",
    "\n",
    "Save the contents of DataFrame a with DataFrameWriter’s jdbc method. For example,\n",
    "\n",
    "    val props = new java.util.Properties()\n",
    "    props.setProperty(\"user\", \"user\")\n",
    "    props.setProperty(\"password\", \"password\")\n",
    "    postsDf.write.jdbc(\"jdbc:postgresql://postgresrv/mydb\", \"persons\", props)\n",
    "    \n",
    "Load the data from RDBMS using DataFrameReader’s jdbc function. For example,\n",
    "\n",
    "    val result = spark.read.jdbc(\"jdbc:postgresql://postgresrv/mydb\", \"persons\", Array(\"viewCount > 3\"), props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* DataFrames translate SQL code and DSL expressions into optimized, low-level RDD operations.\n",
    "* DataFrames have become one of the most important features in Spark and have made Spark SQL the most actively developed Spark component.\n",
    "* Three ways of creating DataFrames exist: by converting existing RDDs, by running SQL queries, or by loading external data.\n",
    "* You can use DataFrame DSL operations to select, filter, group, and join data.\n",
    "* DataFrames support scalar, aggregate, window, and user-defined functions.\n",
    "* With the DataFrameNaFunctions class, accessible through DataFrame’s na field, you can deal with missing values in the dataset.\n",
    "* SparkSQL has its own configuration method.\n",
    "* Tables can be registered temporarily and permanently in the Hive metastore, which can reside in a local Derby database or in a remote relational database.\n",
    "* The Spark SQL shell can be used to directly write queries referencing tables registered in the Hive metastore.\n",
    "* Spark includes a Thrift server that clients can connect to over JDBC and ODBC and use to perform SQL queries.\n",
    "* Data is loaded into DataFrames through DataFrameReader, available through SparkSession’s read field.\n",
    "* Data is saved from DataFrames through DataFrameWriter, available through DataFrame’s write field.\n",
    "* Spark’s built-in data sources are JSON, ORC, Parquet, and JDBC. Third-party data sources are available for download.\n",
    "* Catalyst optimizer (the brain behind DataFrames) can optimize logical plans and create physical execution plans.\n",
    "* The Tungsten project introduced numerous performance improvements through binary, cache-friendly encoding of objects, on-heap and off-heap allocation, and a new shuffle manager.\n",
    "* DataSets are an experimental feature similar to DataFrames, but they enable you to store plain Java objects instead of generic Row containers.\n",
    "* See important [classes of Spark SQL and DataFrames](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
