{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 2 Concepts\n",
    "\n",
    "* SparkContext vs. SparkSession vs. SQLContext\n",
    "* RDD vs. DataFrame vs Dataset\n",
    "\n",
    "__Learning Objectives__\n",
    "\n",
    "* SparkContext vs. SparkSession - how to access these variables in pyspark / jupyter\n",
    "* Understanding RDDs vs. DataFrames vs. Datasets\n",
    "* Working with RDDs and DataFrames\n",
    "* Interoperability between RDD and DataFrames\n",
    "* Understanding the SQLContext\n",
    "* Accessing RDDs in DataFrames\n",
    "* Understanding Spark DataFrame vs. Python Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext vs. SparkSession\n",
    "\n",
    "__SparkContext__\n",
    "\t\n",
    "* Available from Spark 1.x\n",
    "* Familiar code entry point to Spark --- sc = new sparkContext(...) \n",
    "* One SparkContext per application\n",
    "* Create RDDs, accumulators... \n",
    "* Run jobs\n",
    "\n",
    "__SparkSession__\n",
    "\n",
    "* In Spark 2.x, SparkContext is wrapped in SparkSession\n",
    "* Entry point to SparkSQL\n",
    "* Merges SQLContext and HiveContext\n",
    "* Can have multiple SparkSession objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://TirthalP-Laptop.cybage.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SparkContext is initialized and available as \"sc\" in pyspark\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://TirthalP-Laptop.cybage.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x5f59710>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SparkSession is initialized and available as \"spark\" in pyspark\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding RDDs vs. DataFrames vs. Datasets\n",
    "\n",
    "__RDDs__ = Lower-level API\n",
    "\n",
    "* Resilient Distributed Dataset\n",
    "* Primary abstraction since initial version of Spark\n",
    "* Transformations (map, filter…) and Actions on Data (collect, count, reduce…)\n",
    "* Present in Scala, Java, Python and R\n",
    "* From a developer standpoint - deal with unstructured data, complex data types, manage low level details, fine tune performance\n",
    "\n",
    "__DataFrames__ = Untyped Higher-level API\n",
    "\n",
    "* Added to Spark in 1.3\n",
    "* DataFrames built on top of RDDs, and is a dataset organized into named columns\n",
    "* Conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood\n",
    "* Present in Scala, Java, Python and R\n",
    "* From a developer standpoint - deal with structured or semi-structured data, think in SQL, no type safety at compile-time, leverage Spark optimizations for performance\n",
    "\n",
    "__DataSets__ = Typed Higher-level API\n",
    "\n",
    "* Added to Spark in 1.6\n",
    "* Extension of DataFrames: compile-time type safety, OOP interface\n",
    "* No named columns\n",
    "* Present in Scala and Java, but not in Python and R\n",
    "* From a developer standpoint - deal with structured or semi-structured data, functional APIs, offers the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine\n",
    "\n",
    "__Performance__ = RDD < Dataset < DataFrame\n",
    "\n",
    "__Starting Spark 2.0, APIs for Datasets and DataFrames have merged__\n",
    "\n",
    "* Datasets = Datasets of the Row() object in Scala / Java often called DataFrames\n",
    "* DataFrames = Equivalent to Dataset[Row] in Scala / Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with RDDs and DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:184"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RDD containing some mixed data\n",
    "mixed_data = sc.parallelize([1, \"Tirthal\", \"Patel\", 37, 58])\n",
    "mixed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'Tirthal', 'Patel', 37, 58]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform action on RDD, e.g. view all the elements from the RDD \n",
    "mixed_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try to convert mixed data RDD to DataFrame using rdd.toDF()__\n",
    "\n",
    "Why giving TypeError: Can not infer schema for type? \n",
    "\n",
    "Well, this RDD has no schema and contains elements of different types, so it cannot be converted to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f466725c3dac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmixed_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Softwares\\spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mtoDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \"\"\"\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    382\u001b[0m         \"\"\"\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msamplingRatio\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m             \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[1;34m(row, names)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1094\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1095\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m     \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not infer schema for type: <class 'int'>"
     ]
    }
   ],
   "source": [
    "df = mixed_data.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interoperability between RDD and DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:184"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RDD with structured list\n",
    "usersRDD = sc.parallelize([[1, \"Tirthal\", \"Patel\", 37, 58], [2, \"Ian\", \"Patel\", 8, 20]])\n",
    "usersRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'Tirthal', 'Patel', 37, 58], [2, 'Ian', 'Patel', 8, 20]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform action on RDD, e.g. view all the elements from the RDD \n",
    "usersRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'Tirthal', 'Patel', 37, 58]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform action on RDD, e.g. get first record fromt the RDD\n",
    "usersRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try to convert structured data RDD to DataFrame using rdd.toDF()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: bigint, _2: string, _3: string, _4: bigint, _5: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usersDF = usersRDD.toDF()\n",
    "usersDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+---+---+\n",
      "| _1|     _2|   _3| _4| _5|\n",
      "+---+-------+-----+---+---+\n",
      "|  1|Tirthal|Patel| 37| 58|\n",
      "|  2|    Ian|Patel|  8| 20|\n",
      "+---+-------+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF.show()\n",
    "\n",
    "# See column names have been automatically generated and assigned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How to specify names for each field instead of auto-generated?__\n",
    "\n",
    "Structured data with a schema can be converted to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[19] at parallelize at PythonRDD.scala:184"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import Row \n",
    "\n",
    "usersData = sc.parallelize([Row(id=1, fname=\"Tirthal\", lname=\"Patel\",age=37, weight=58.00),\n",
    "                            Row(id=2, fname=\"Ian\", lname=\"Patel\",age=8, weight=20.00)])\n",
    "usersData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=37, fname='Tirthal', id=1, lname='Patel', weight=58.0),\n",
       " Row(age=8, fname='Ian', id=2, lname='Patel', weight=20.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usersData.collect()\n",
    "\n",
    "# See RDD is made up of Row objects - each element is Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----+------+\n",
      "|age|  fname| id|lname|weight|\n",
      "+---+-------+---+-----+------+\n",
      "| 37|Tirthal|  1|Patel|  58.0|\n",
      "|  8|    Ian|  2|Patel|  20.0|\n",
      "+---+-------+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDataFrame = usersData.toDF()\n",
    "usersDataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Does it support complex data types?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------+---------+-------------------+\n",
      "|            col_dict|col_float|    col_list|  col_row|           col_time|\n",
      "+--------------------+---------+------------+---------+-------------------+\n",
      "|[K1 -> 100, K2 ->...|    12.22|[10, 20, 30]|[25, 100]|2018-06-08 11:02:08|\n",
      "|[K1 -> 200, K2 ->...|    80.25|[50, 60, 70]|[45, 600]|2017-05-09 18:10:45|\n",
      "+--------------------+---------+------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "complicated_data = sc.parallelize([Row(col_float=12.22, \n",
    "                                   col_time=datetime(2018, 6, 8, 11, 2, 8), \n",
    "                                   col_row=Row(x=25, y=100), \n",
    "                                   col_dict={ \"K1\" : 100, \"K2\" : 200},\n",
    "                                   col_list=[10, 20, 30]\n",
    "                                 ),                                                                      \n",
    "                                 Row(col_float=80.25, \n",
    "                                   col_time=datetime(2017, 5, 9, 18, 10, 45), \n",
    "                                   col_row=Row(x=45, y=600), \n",
    "                                   col_dict={ \"K1\" : 200, \"K2\" : 300},\n",
    "                                   col_list=[50, 60, 70]\n",
    "                                 )])\n",
    "\n",
    "complicated_data_df = complicated_data.toDF()\n",
    "complicated_data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the SQLContext\n",
    "\n",
    "__SQLContext__\n",
    "\n",
    "* The entry point into all functionality in Spark SQL\n",
    "* Just a wrapper around SparkContext, which enables to run sql queries on spark data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x76d8f60>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate single id column DataFrame\n",
    "ids_df = sqlContext.range(3)\n",
    "ids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ids_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create DataFrame for any complex data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_list = [(1, 'Square', 'red'), (2, 'Circle', 'blue'), (3, 'Rectangle', 'black')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----+\n",
      "| _1|       _2|   _3|\n",
      "+---+---------+-----+\n",
      "|  1|   Square|  red|\n",
      "|  2|   Circle| blue|\n",
      "|  3|Rectangle|black|\n",
      "+---+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame with auto-generated column names\n",
    "sqlContext.createDataFrame(shapes_list).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----+\n",
      "| ID|    SHAPE|COLOR|\n",
      "+---+---------+-----+\n",
      "|  1|   Square|  red|\n",
      "|  2|   Circle| blue|\n",
      "|  3|Rectangle|black|\n",
      "+---+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame with custom specification of data\n",
    "sqlContext.createDataFrame(shapes_list, ['ID', 'SHAPE', 'COLOR']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Use SQLContext to generate DataFrames from RDD__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[65] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RDD of Row objects without column specification\n",
    "shapes_rows_RDD = sc.parallelize([Row(1, 'Square', 'red'), Row(2, 'Circle', 'blue'), Row(3, 'Rectangle', 'black')])\n",
    "\n",
    "# Setup column names for Row objects\n",
    "shapes_column_names =  Row('ID', 'SHAPE', 'COLOR')\n",
    "\n",
    "# Use map operation to perform transformation on every element in the RDD (i.e. assign column names to the data)\n",
    "shapes_RDD = shapes_rows_RDD.map(lambda r : shapes_column_names(*r))\n",
    "shapes_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=1, SHAPE='Square', COLOR='red'),\n",
       " Row(ID=2, SHAPE='Circle', COLOR='blue'),\n",
       " Row(ID=3, SHAPE='Rectangle', COLOR='black')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes_RDD.collect()\n",
    "\n",
    "# See field names are assigned to row objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: bigint, SHAPE: string, COLOR: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame from RDD using SQLContext\n",
    "shapes_DF = sqlContext.createDataFrame(shapes_RDD)\n",
    "shapes_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----+\n",
      "| ID|    SHAPE|COLOR|\n",
      "+---+---------+-----+\n",
      "|  1|   Square|  red|\n",
      "|  2|   Circle| blue|\n",
      "|  3|Rectangle|black|\n",
      "+---+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shapes_DF.show()\n",
    "\n",
    "# See this inferred the data types for each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing RDDs in DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Access particular data from the complex data frame using metrics notation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col_dict={'K1': 100, 'K2': 200}, col_float=12.22, col_list=[10, 20, 30], col_row=Row(x=25, y=100), col_time=datetime.datetime(2018, 6, 8, 11, 2, 8)),\n",
       " Row(col_dict={'K1': 200, 'K2': 300}, col_float=80.25, col_list=[50, 60, 70], col_row=Row(x=45, y=600), col_time=datetime.datetime(2017, 5, 9, 18, 10, 45))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complicated_data_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 20, 30]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of 1st row and 3rd column\n",
    "row_first_col_list_value = complicated_data_df.collect()[0][2]\n",
    "row_first_col_list_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 20, 30, 40]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_first_col_list_value.append(40)\n",
    "row_first_col_list_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(col_dict={'K1': 100, 'K2': 200}, col_float=12.22, col_list=[10, 20, 30], col_row=Row(x=25, y=100), col_time=datetime.datetime(2018, 6, 8, 11, 2, 8))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the original cell remains unchanged, because copy of the list was returned by data frame\n",
    "complicated_data_df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract specific columns by converting the DataFrame to RDD__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.datetime(2018, 6, 8, 11, 2, 8), Row(x=25, y=100)),\n",
       " (datetime.datetime(2017, 5, 9, 18, 10, 45), Row(x=45, y=600))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complicated_data_df.rdd\\\n",
    "                   .map(lambda x : (x.col_time, x.col_row))\\\n",
    "                   .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract specific columns using select method of DataFrame instead of rdd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|           col_time|  col_row|\n",
      "+-------------------+---------+\n",
      "|2018-06-08 11:02:08|[25, 100]|\n",
      "|2017-05-09 18:10:45|[45, 600]|\n",
      "+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complicated_data_df.select('col_time', 'col_row').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How to calculate some value on every record of DataFrame?__\n",
    "\n",
    "DataFrame doesn't support map operation, rather withColumn can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|col_float|col_twofold_float|\n",
      "+---------+-----------------+\n",
      "|    12.22|            24.44|\n",
      "|    80.25|            160.5|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complicated_data_df.select('col_float')\\\n",
    "                   .withColumn(\"col_twofold_float\", complicated_data_df.col_float * 2)\\\n",
    "                   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How to rename column name within DataFrame?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------+---------+-------------------+\n",
      "|            col_dict|col_float|    col_list|  col_row|       col_datetime|\n",
      "+--------------------+---------+------------+---------+-------------------+\n",
      "|[K1 -> 100, K2 ->...|    12.22|[10, 20, 30]|[25, 100]|2018-06-08 11:02:08|\n",
      "|[K1 -> 200, K2 ->...|    80.25|[50, 60, 70]|[45, 600]|2017-05-09 18:10:45|\n",
      "+--------------------+---------+------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complicated_data_df.withColumnRenamed(\"col_time\", \"col_datetime\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------+---------+-------------------+\n",
      "|            col_dict|col_float|    col_list|  col_row|           col_time|\n",
      "+--------------------+---------+------------+---------+-------------------+\n",
      "|[K1 -> 100, K2 ->...|    12.22|[10, 20, 30]|[25, 100]|2018-06-08 11:02:08|\n",
      "|[K1 -> 200, K2 ->...|    80.25|[50, 60, 70]|[45, 600]|2017-05-09 18:10:45|\n",
      "+--------------------+---------+------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Original DataFrame is NOT changed, rather new DataFrame is created and returned (see above)\n",
    "complicated_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|        Date & Time|\n",
      "+-------------------+\n",
      "|2018-06-08 11:02:08|\n",
      "|2017-05-09 18:10:45|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How to get specific column data with alias column name\n",
    "complicated_data_df.select(complicated_data_df.col_time.alias(\"Date & Time\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame vs. Python Pandas DataFrame\n",
    "\n",
    "* __Spark DataFrame__ = Distributed across machines as per Spark architecture\n",
    "* __Pandas DataFrame__ = In memory on single machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert from Spark DataFrame to Pandas DataFrame__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_dict</th>\n",
       "      <th>col_float</th>\n",
       "      <th>col_list</th>\n",
       "      <th>col_row</th>\n",
       "      <th>col_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'K1': 100, 'K2': 200}</td>\n",
       "      <td>12.22</td>\n",
       "      <td>[10, 20, 30]</td>\n",
       "      <td>(25, 100)</td>\n",
       "      <td>2018-06-08 11:02:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'K1': 200, 'K2': 300}</td>\n",
       "      <td>80.25</td>\n",
       "      <td>[50, 60, 70]</td>\n",
       "      <td>(45, 600)</td>\n",
       "      <td>2017-05-09 18:10:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 col_dict  col_float      col_list    col_row  \\\n",
       "0  {'K1': 100, 'K2': 200}      12.22  [10, 20, 30]  (25, 100)   \n",
       "1  {'K1': 200, 'K2': 300}      80.25  [50, 60, 70]  (45, 600)   \n",
       "\n",
       "             col_time  \n",
       "0 2018-06-08 11:02:08  \n",
       "1 2017-05-09 18:10:45  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "df_pandas = complicated_data_df.toPandas()\n",
    "df_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert from Pandas DataFrame to Spark DataFrame__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = sqlContext.createDataFrame(df_pandas).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame vs. Spark Dataset\n",
    "\n",
    "* __Spark DataFrame__ = Untyped Dataset Operations.\n",
    "* __Spark Dataset__ = Datasets are similar to RDDs, however, instead of using Java serialization or Kryo they use a specialized Encoder to serialize the objects for processing or transmitting over the network. While both encoders and standard serialization are responsible for turning an object into bytes, encoders are code generated dynamically and use a format that allows Spark to perform many operations like filtering, sorting and hashing without deserializing the bytes back into an object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating DataFrames and Performing untyped dataset operations__ (example code in Scala)\n",
    "\n",
    "```\n",
    "val df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "    \n",
    "// Print the schema in a tree format\n",
    "df.printSchema()\n",
    "    // root\n",
    "    // |-- age: long (nullable = true)\n",
    "    // |-- name: string (nullable = true)\n",
    "    \n",
    "// Displays the content of the DataFrame to stdout    \n",
    "df.show()\n",
    "    // +----+-------+\n",
    "    // | age|   name|\n",
    "    // +----+-------+\n",
    "    // |null|Michael|\n",
    "    // |  30|   Andy|\n",
    "    // |  19| Justin|\n",
    "    // +----+-------+\n",
    "    \n",
    "// Select only the \"name\" column\n",
    "df.select(\"name\").show()\n",
    "    // +-------+\n",
    "    // |   name|\n",
    "    // +-------+\n",
    "    // |Michael|\n",
    "    // |   Andy|\n",
    "    // | Justin|\n",
    "    // +-------+\n",
    "\n",
    "// Count people by age\n",
    "df.groupBy(\"age\").count().show()\n",
    "    // +----+-----+\n",
    "    // | age|count|\n",
    "    // +----+-----+\n",
    "    // |  19|    1|\n",
    "    // |null|    1|\n",
    "    // |  30|    1|\n",
    "    // +----+-----+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating Datasets__ (example code in Scala)\n",
    "\n",
    "```\n",
    "case class Person(name: String, age: Long)\n",
    "\n",
    "// Encoders are created for case classes\n",
    "val caseClassDS = Seq(Person(\"Andy\", 32)).toDS()\n",
    "caseClassDS.show()\n",
    "    // +----+---+\n",
    "    // |name|age|\n",
    "    // +----+---+\n",
    "    // |Andy| 32|\n",
    "    // +----+---+\n",
    "\n",
    "// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name\n",
    "val path = \"examples/src/main/resources/people.json\"\n",
    "val peopleDS = spark.read.json(path).as[Person]\n",
    "peopleDS.show()\n",
    "    // +----+-------+\n",
    "    // | age|   name|\n",
    "    // +----+-------+\n",
    "    // |null|Michael|\n",
    "    // |  30|   Andy|\n",
    "    // |  19| Justin|\n",
    "    // +----+-------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above notes and code I produced for my own learning purpose, while attended [pluralsight course](https://app.pluralsight.com/library/courses/spark-2-getting-started/table-of-contents)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
